{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Evaluation with Confusion Matrix\n",
    "\n",
    "This notebook demonstrates how to evaluate collaborative filtering algorithms using confusion matrix metrics by treating the problem as a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our collaborative filtering algorithm\n",
    "from algorithm import get_recommendations, CollaborativeFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movie Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movie dataset\n",
    "df = pd.read_csv('dataset/movies.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample User Ratings for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample user ratings for evaluation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Select a subset of movies for testing\n",
    "test_movies = df.sample(100, random_state=42)\n",
    "movie_ids = test_movies['id'].astype(str).tolist()\n",
    "\n",
    "# Create sample user ratings (simulate multiple users)\n",
    "def create_sample_ratings(movie_ids: List[str], n_users: int = 5) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Create sample user ratings for testing.\"\"\"\n",
    "    users_ratings = {}\n",
    "    \n",
    "    for user_id in range(n_users):\n",
    "        user_name = f\"user_{user_id}\"\n",
    "        user_ratings = {}\n",
    "        \n",
    "        # Each user rates 20-40 movies randomly\n",
    "        n_ratings = np.random.randint(20, min(41, len(movie_ids)))\n",
    "        rated_movies = np.random.choice(movie_ids, n_ratings, replace=False)\n",
    "        \n",
    "        for movie_id in rated_movies:\n",
    "            # Generate ratings with some preference patterns\n",
    "            rating = np.random.choice([1,2,3,4,5,6,7,8,9,10], \n",
    "                                    p=[0.05,0.05,0.1,0.1,0.15,0.15,0.2,0.1,0.05,0.05])\n",
    "            user_ratings[movie_id] = rating\n",
    "            \n",
    "        users_ratings[user_name] = user_ratings\n",
    "    \n",
    "    return users_ratings\n",
    "\n",
    "# Create sample data\n",
    "sample_users = create_sample_ratings(movie_ids, n_users=5)\n",
    "print(f\"Created ratings for {len(sample_users)} users\")\n",
    "for user, ratings in sample_users.items():\n",
    "    print(f\"{user}: {len(ratings)} ratings, avg: {np.mean(list(ratings.values())):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Method 1: Rating Prediction as Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction_multiclass(df: pd.DataFrame, \n",
    "                                         train_ratings: Dict[str, int],\n",
    "                                         test_ratings: Dict[str, int]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Evaluate collaborative filtering as multi-class classification.\n",
    "    Classes: Poor (1-3), Average (4-6), Good (7-10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def rating_to_class(rating: int) -> str:\n",
    "        if rating <= 3:\n",
    "            return \"Poor\"\n",
    "        elif rating <= 6:\n",
    "            return \"Average\" \n",
    "        else:\n",
    "            return \"Good\"\n",
    "    \n",
    "    # Get recommendations based on training data\n",
    "    recommender = CollaborativeFilter(df)\n",
    "    recommender.update_user_ratings(train_ratings)\n",
    "    \n",
    "    # For each test movie, predict its class based on recommendation score\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for movie_id, actual_rating in test_ratings.items():\n",
    "        # Get movie data\n",
    "        movie_row = df[df['id'].astype(str) == str(movie_id)]\n",
    "        if movie_row.empty:\n",
    "            continue\n",
    "            \n",
    "        movie = movie_row.iloc[0]\n",
    "        \n",
    "        # Calculate recommendation score\n",
    "        rated_movies, user_avg_rating, top_genres, genre_scores = recommender._get_user_preferences()\n",
    "        if not top_genres:\n",
    "            predicted_class = \"Average\"  # Default prediction\n",
    "        else:\n",
    "            score = recommender._calculate_movie_score(movie, top_genres, genre_scores, user_avg_rating)\n",
    "            \n",
    "            # Convert score to class prediction\n",
    "            if score >= 8.0:\n",
    "                predicted_class = \"Good\"\n",
    "            elif score >= 5.0:\n",
    "                predicted_class = \"Average\"\n",
    "            else:\n",
    "                predicted_class = \"Poor\"\n",
    "        \n",
    "        y_true.append(rating_to_class(actual_rating))\n",
    "        y_pred.append(predicted_class)\n",
    "    \n",
    "    if not y_true:\n",
    "        return {\"error\": \"No valid predictions made\"}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    labels = [\"Poor\", \"Average\", \"Good\"]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    report = classification_report(y_true, y_pred, labels=labels, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"labels\": labels,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"classification_report\": report,\n",
    "        \"predictions\": list(zip(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "# Test with one user\n",
    "test_user = \"user_0\"\n",
    "user_ratings = sample_users[test_user]\n",
    "\n",
    "# Split ratings into train/test (80/20)\n",
    "movie_list = list(user_ratings.keys())\n",
    "np.random.shuffle(movie_list)\n",
    "split_point = int(0.8 * len(movie_list))\n",
    "\n",
    "train_ratings = {k: user_ratings[k] for k in movie_list[:split_point]}\n",
    "test_ratings = {k: user_ratings[k] for k in movie_list[split_point:]}\n",
    "\n",
    "print(f\"Train set: {len(train_ratings)} movies\")\n",
    "print(f\"Test set: {len(test_ratings)} movies\")\n",
    "\n",
    "# Evaluate\n",
    "results_multiclass = evaluate_rating_prediction_multiclass(df, train_ratings, test_ratings)\n",
    "print(f\"\\nMulti-class Classification Results:\")\n",
    "print(f\"Accuracy: {results_multiclass['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix for multi-class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(results_multiclass['confusion_matrix'], \n",
    "            annot=True, fmt='d', \n",
    "            xticklabels=results_multiclass['labels'], \n",
    "            yticklabels=results_multiclass['labels'],\n",
    "            cmap='Blues')\n",
    "plt.title('Confusion Matrix - Multi-class Rating Prediction')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "for label in results_multiclass['labels']:\n",
    "    metrics = results_multiclass['classification_report'][label]\n",
    "    print(f\"{label:>8}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Method 2: Binary Classification (Good vs Not Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_classification(df: pd.DataFrame, \n",
    "                                 train_ratings: Dict[str, int],\n",
    "                                 test_ratings: Dict[str, int],\n",
    "                                 threshold: int = 7) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Evaluate collaborative filtering as binary classification.\n",
    "    Positive class: Rating >= threshold (Good movies)\n",
    "    Negative class: Rating < threshold (Not good movies)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get recommendations based on training data\n",
    "    recommender = CollaborativeFilter(df)\n",
    "    recommender.update_user_ratings(train_ratings)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for movie_id, actual_rating in test_ratings.items():\n",
    "        # Get movie data\n",
    "        movie_row = df[df['id'].astype(str) == str(movie_id)]\n",
    "        if movie_row.empty:\n",
    "            continue\n",
    "            \n",
    "        movie = movie_row.iloc[0]\n",
    "        \n",
    "        # Calculate recommendation score\n",
    "        rated_movies, user_avg_rating, top_genres, genre_scores = recommender._get_user_preferences()\n",
    "        if not top_genres:\n",
    "            predicted_good = 0  # Default: not good\n",
    "        else:\n",
    "            score = recommender._calculate_movie_score(movie, top_genres, genre_scores, user_avg_rating)\n",
    "            \n",
    "            # Convert score to binary prediction\n",
    "            predicted_good = 1 if score >= 6.0 else 0  # Threshold for \"good\" recommendation\n",
    "        \n",
    "        actual_good = 1 if actual_rating >= threshold else 0\n",
    "        \n",
    "        y_true.append(actual_good)\n",
    "        y_pred.append(predicted_good)\n",
    "    \n",
    "    if not y_true:\n",
    "        return {\"error\": \"No valid predictions made\"}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"threshold\": threshold,\n",
    "        \"predictions\": list(zip(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "# Evaluate binary classification\n",
    "results_binary = evaluate_binary_classification(df, train_ratings, test_ratings, threshold=7)\n",
    "\n",
    "print(\"Binary Classification Results (Good vs Not Good):\")\n",
    "print(f\"Threshold: {results_binary['threshold']} stars\")\n",
    "print(f\"Accuracy: {results_binary['accuracy']:.3f}\")\n",
    "print(f\"Precision: {results_binary['precision']:.3f}\")\n",
    "print(f\"Recall: {results_binary['recall']:.3f}\")\n",
    "print(f\"F1-Score: {results_binary['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binary confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(results_binary['confusion_matrix'], \n",
    "            annot=True, fmt='d', \n",
    "            xticklabels=['Not Good', 'Good'], \n",
    "            yticklabels=['Not Good', 'Good'],\n",
    "            cmap='Blues')\n",
    "plt.title('Confusion Matrix - Binary Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics from confusion matrix\n",
    "tn, fp, fn, tp = results_binary['confusion_matrix'].ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "print(f\"\\nSpecificity (True Negative Rate): {specificity:.3f}\")\n",
    "print(f\"Sensitivity (True Positive Rate/Recall): {results_binary['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Method 3: Recommendation as Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendation_binary(df: pd.DataFrame, \n",
    "                                 train_ratings: Dict[str, int],\n",
    "                                 test_ratings: Dict[str, int],\n",
    "                                 n_recommendations: int = 10,\n",
    "                                 threshold: int = 7) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Evaluate collaborative filtering recommendations as binary classification.\n",
    "    Positive: Movie is in top-N recommendations\n",
    "    Negative: Movie is not in top-N recommendations\n",
    "    Ground truth based on actual ratings >= threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get recommendations based on training data\n",
    "    recommended_movies = get_recommendations(df, train_ratings, n_recommendations)\n",
    "    recommended_ids = set(recommended_movies['id'].astype(str).tolist())\n",
    "    \n",
    "    y_true = []  # 1 if user actually liked the movie (rating >= threshold)\n",
    "    y_pred = []  # 1 if movie was recommended\n",
    "    \n",
    "    # Evaluate all test movies\n",
    "    for movie_id, actual_rating in test_ratings.items():\n",
    "        actual_liked = 1 if actual_rating >= threshold else 0\n",
    "        was_recommended = 1 if movie_id in recommended_ids else 0\n",
    "        \n",
    "        y_true.append(actual_liked)\n",
    "        y_pred.append(was_recommended)\n",
    "    \n",
    "    if not y_true:\n",
    "        return {\"error\": \"No valid predictions made\"}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"n_recommendations\": n_recommendations,\n",
    "        \"threshold\": threshold,\n",
    "        \"recommended_movies\": recommended_movies,\n",
    "        \"predictions\": list(zip(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "# Evaluate recommendation system\n",
    "results_recommendation = evaluate_recommendation_binary(df, train_ratings, test_ratings, \n",
    "                                                      n_recommendations=5, threshold=7)\n",
    "\n",
    "print(\"Recommendation System Evaluation:\")\n",
    "print(f\"Number of recommendations: {results_recommendation['n_recommendations']}\")\n",
    "print(f\"Threshold for 'liked': {results_recommendation['threshold']} stars\")\n",
    "print(f\"Accuracy: {results_recommendation['accuracy']:.3f}\")\n",
    "print(f\"Precision: {results_recommendation['precision']:.3f}\")\n",
    "print(f\"Recall: {results_recommendation['recall']:.3f}\")\n",
    "print(f\"F1-Score: {results_recommendation['f1_score']:.3f}\")\n",
    "\n",
    "# Show recommended movies\n",
    "print(f\"\\nRecommended Movies:\")\n",
    "for _, movie in results_recommendation['recommended_movies'].head().iterrows():\n",
    "    print(f\"- {movie['title']} (Rating: {movie['vote_average']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize recommendation confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(results_recommendation['confusion_matrix'], \n",
    "            annot=True, fmt='d', \n",
    "            xticklabels=['Not Recommended', 'Recommended'], \n",
    "            yticklabels=['Not Liked', 'Liked'],\n",
    "            cmap='Blues')\n",
    "plt.title('Confusion Matrix - Recommendation System')\n",
    "plt.ylabel('True Label (User Preference)')\n",
    "plt.xlabel('Predicted Label (System Recommendation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-User Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_users(df: pd.DataFrame, users_ratings: Dict[str, Dict[str, int]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate collaborative filtering across multiple users.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for user_id, user_ratings in users_ratings.items():\n",
    "        if len(user_ratings) < 10:  # Skip users with too few ratings\n",
    "            continue\n",
    "            \n",
    "        # Split ratings\n",
    "        movie_list = list(user_ratings.keys())\n",
    "        np.random.shuffle(movie_list)\n",
    "        split_point = int(0.8 * len(movie_list))\n",
    "        \n",
    "        train_ratings = {k: user_ratings[k] for k in movie_list[:split_point]}\n",
    "        test_ratings = {k: user_ratings[k] for k in movie_list[split_point:]}\n",
    "        \n",
    "        if len(test_ratings) < 3:  # Need at least 3 test ratings\n",
    "            continue\n",
    "        \n",
    "        # Evaluate binary classification\n",
    "        binary_results = evaluate_binary_classification(df, train_ratings, test_ratings)\n",
    "        \n",
    "        # Evaluate recommendation system\n",
    "        rec_results = evaluate_recommendation_binary(df, train_ratings, test_ratings, \n",
    "                                                   n_recommendations=5)\n",
    "        \n",
    "        results.append({\n",
    "            'user_id': user_id,\n",
    "            'n_train': len(train_ratings),\n",
    "            'n_test': len(test_ratings),\n",
    "            'binary_accuracy': binary_results['accuracy'],\n",
    "            'binary_precision': binary_results['precision'],\n",
    "            'binary_recall': binary_results['recall'],\n",
    "            'binary_f1': binary_results['f1_score'],\n",
    "            'rec_accuracy': rec_results['accuracy'],\n",
    "            'rec_precision': rec_results['precision'],\n",
    "            'rec_recall': rec_results['recall'],\n",
    "            'rec_f1': rec_results['f1_score']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate all users\n",
    "all_results = evaluate_all_users(df, sample_users)\n",
    "print(\"Cross-User Evaluation Results:\")\n",
    "print(all_results)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"Binary Classification:\")\n",
    "print(f\"  Mean Accuracy: {all_results['binary_accuracy'].mean():.3f} ± {all_results['binary_accuracy'].std():.3f}\")\n",
    "print(f\"  Mean Precision: {all_results['binary_precision'].mean():.3f} ± {all_results['binary_precision'].std():.3f}\")\n",
    "print(f\"  Mean Recall: {all_results['binary_recall'].mean():.3f} ± {all_results['binary_recall'].std():.3f}\")\n",
    "print(f\"  Mean F1-Score: {all_results['binary_f1'].mean():.3f} ± {all_results['binary_f1'].std():.3f}\")\n",
    "\n",
    "print(\"\\nRecommendation System:\")\n",
    "print(f\"  Mean Accuracy: {all_results['rec_accuracy'].mean():.3f} ± {all_results['rec_accuracy'].std():.3f}\")\n",
    "print(f\"  Mean Precision: {all_results['rec_precision'].mean():.3f} ± {all_results['rec_precision'].std():.3f}\")\n",
    "print(f\"  Mean Recall: {all_results['rec_recall'].mean():.3f} ± {all_results['rec_recall'].std():.3f}\")\n",
    "print(f\"  Mean F1-Score: {all_results['rec_f1'].mean():.3f} ± {all_results['rec_f1'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].boxplot([all_results['binary_accuracy'], all_results['rec_accuracy']], \n",
    "                   labels=['Binary Classification', 'Recommendation System'])\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "\n",
    "# Precision comparison\n",
    "axes[0, 1].boxplot([all_results['binary_precision'], all_results['rec_precision']], \n",
    "                   labels=['Binary Classification', 'Recommendation System'])\n",
    "axes[0, 1].set_title('Precision Comparison')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "\n",
    "# Recall comparison\n",
    "axes[1, 0].boxplot([all_results['binary_recall'], all_results['rec_recall']], \n",
    "                   labels=['Binary Classification', 'Recommendation System'])\n",
    "axes[1, 0].set_title('Recall Comparison')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[1, 1].boxplot([all_results['binary_f1'], all_results['rec_f1']], \n",
    "                   labels=['Binary Classification', 'Recommendation System'])\n",
    "axes[1, 1].set_title('F1-Score Comparison')\n",
    "axes[1, 1].set_ylabel('F1-Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Metrics: Precision@K and Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(df: pd.DataFrame, \n",
    "                            train_ratings: Dict[str, int],\n",
    "                            test_ratings: Dict[str, int],\n",
    "                            k_values: List[int] = [1, 3, 5, 10],\n",
    "                            threshold: int = 7) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Calculate Precision@K for different values of K.\n",
    "    \"\"\"\n",
    "    precision_at_k = {}\n",
    "    \n",
    "    # Get liked movies from test set\n",
    "    liked_movies = set([movie_id for movie_id, rating in test_ratings.items() if rating >= threshold])\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Get top-K recommendations\n",
    "        recommendations = get_recommendations(df, train_ratings, k)\n",
    "        recommended_ids = set(recommendations['id'].astype(str).tolist())\n",
    "        \n",
    "        # Calculate precision@k\n",
    "        if recommended_ids:\n",
    "            relevant_recommendations = recommended_ids.intersection(liked_movies)\n",
    "            precision_at_k[k] = len(relevant_recommendations) / len(recommended_ids)\n",
    "        else:\n",
    "            precision_at_k[k] = 0.0\n",
    "    \n",
    "    return precision_at_k\n",
    "\n",
    "def calculate_recall_at_k(df: pd.DataFrame, \n",
    "                         train_ratings: Dict[str, int],\n",
    "                         test_ratings: Dict[str, int],\n",
    "                         k_values: List[int] = [1, 3, 5, 10],\n",
    "                         threshold: int = 7) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Calculate Recall@K for different values of K.\n",
    "    \"\"\"\n",
    "    recall_at_k = {}\n",
    "    \n",
    "    # Get liked movies from test set\n",
    "    liked_movies = set([movie_id for movie_id, rating in test_ratings.items() if rating >= threshold])\n",
    "    \n",
    "    if not liked_movies:\n",
    "        return {k: 0.0 for k in k_values}\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Get top-K recommendations\n",
    "        recommendations = get_recommendations(df, train_ratings, k)\n",
    "        recommended_ids = set(recommendations['id'].astype(str).tolist())\n",
    "        \n",
    "        # Calculate recall@k\n",
    "        relevant_recommendations = recommended_ids.intersection(liked_movies)\n",
    "        recall_at_k[k] = len(relevant_recommendations) / len(liked_movies)\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "# Calculate Precision@K and Recall@K for test user\n",
    "k_values = [1, 3, 5, 10]\n",
    "precision_at_k = calculate_precision_at_k(df, train_ratings, test_ratings, k_values)\n",
    "recall_at_k = calculate_recall_at_k(df, train_ratings, test_ratings, k_values)\n",
    "\n",
    "print(\"Precision@K and Recall@K Results:\")\n",
    "for k in k_values:\n",
    "    print(f\"K={k}: Precision@{k}={precision_at_k[k]:.3f}, Recall@{k}={recall_at_k[k]:.3f}\")\n",
    "\n",
    "# Plot Precision@K and Recall@K\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, [precision_at_k[k] for k in k_values], 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('Precision@K')\n",
    "plt.xlabel('K (Number of Recommendations)')\n",
    "plt.ylabel('Precision@K')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, [recall_at_k[k] for k in k_values], 'ro-', linewidth=2, markersize=8)\n",
    "plt.title('Recall@K')\n",
    "plt.xlabel('K (Number of Recommendations)')\n",
    "plt.ylabel('Recall@K')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrates three different approaches to evaluate collaborative filtering algorithms using confusion matrix metrics:\n",
    "\n",
    "1. **Multi-class Classification**: Converting ratings to classes (Poor/Average/Good)\n",
    "2. **Binary Classification**: Good vs Not Good movies\n",
    "3. **Recommendation Evaluation**: Whether recommended movies were actually liked\n",
    "\n",
    "Key metrics calculated:\n",
    "- **Confusion Matrix**: Shows true vs predicted classifications\n",
    "- **Accuracy**: Overall correctness of predictions\n",
    "- **Precision**: Of predicted positives, how many were actually positive\n",
    "- **Recall**: Of actual positives, how many were predicted positive\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Precision@K**: Precision of top-K recommendations\n",
    "- **Recall@K**: Recall of top-K recommendations\n",
    "\n",
    "These evaluation methods help assess how well the collaborative filtering algorithm predicts user preferences and makes relevant recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}