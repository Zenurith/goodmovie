{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System Evaluation\n",
    "\n",
    "This notebook evaluates the performance of our collaborative filtering recommendation system using various metrics including:\n",
    "- **Rating Prediction Metrics**: MAE, RMSE\n",
    "- **Ranking Metrics**: Precision@K, Recall@K, F1@K, NDCG@K\n",
    "- **Coverage and Diversity Metrics**\n",
    "- **User Similarity Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📊 Evaluation Environment Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our recommendation system\n",
    "import sys\n",
    "sys.path.append('./algorithm')\n",
    "\n",
    "from algorithm.collaborative_filtering import (\n",
    "    CollaborativeFilter, \n",
    "    load_users_data, \n",
    "    get_recommendations\n",
    ")\n",
    "\n",
    "print(\"✅ Collaborative Filtering System Imported Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie dataset\n",
    "df = pd.read_csv('dataset/movies.csv')\n",
    "print(f\"📽️ Loaded {len(df):,} movies from dataset\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Load user ratings data\n",
    "users_data = load_users_data()\n",
    "print(f\"👥 Loaded ratings from {len(users_data)} users\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n📊 Dataset Overview:\")\n",
    "print(f\"• Movies: {len(df):,}\")\n",
    "print(f\"• Users: {len(users_data)}\")\n",
    "print(f\"• Total ratings: {sum(len(ratings) for ratings in users_data.values()):,}\")\n",
    "print(f\"• Avg ratings per user: {np.mean([len(ratings) for ratings in users_data.values()]):.1f}\")\n",
    "print(f\"• Sparsity: {(1 - sum(len(ratings) for ratings in users_data.values()) / (len(users_data) * len(df))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_evaluation_data(users_data, test_size=0.2, min_ratings=3):\n",
    "    \"\"\"\n",
    "    Split user ratings into train/test sets for evaluation.\n",
    "    Only include users with minimum number of ratings.\n",
    "    \"\"\"\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    valid_users = 0\n",
    "    \n",
    "    for user_id, ratings in users_data.items():\n",
    "        if len(ratings) >= min_ratings:\n",
    "            valid_users += 1\n",
    "            \n",
    "            # Convert to lists for splitting\n",
    "            movies = list(ratings.keys())\n",
    "            scores = list(ratings.values())\n",
    "            \n",
    "            # Split ratings\n",
    "            train_movies, test_movies, train_scores, test_scores = train_test_split(\n",
    "                movies, scores, test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Create dictionaries\n",
    "            train_data[user_id] = dict(zip(train_movies, train_scores))\n",
    "            test_data[user_id] = dict(zip(test_movies, test_scores))\n",
    "    \n",
    "    print(f\"📊 Evaluation Data Prepared:\")\n",
    "    print(f\"• Valid users (≥{min_ratings} ratings): {valid_users}/{len(users_data)}\")\n",
    "    print(f\"• Train ratings: {sum(len(r) for r in train_data.values()):,}\")\n",
    "    print(f\"• Test ratings: {sum(len(r) for r in test_data.values()):,}\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Prepare train/test split\n",
    "train_data, test_data = prepare_evaluation_data(users_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rating Prediction Metrics (MAE, RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction(train_data, test_data, df):\n",
    "    \"\"\"\n",
    "    Evaluate rating prediction accuracy using MAE and RMSE.\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    user_results = []\n",
    "    \n",
    "    print(\"🔮 Evaluating Rating Prediction Accuracy...\")\n",
    "    \n",
    "    for i, (user_id, test_ratings) in enumerate(test_data.items()):\n",
    "        if i % 5 == 0:  # Progress indicator\n",
    "            print(f\"Progress: {i+1}/{len(test_data)} users\")\n",
    "        \n",
    "        # Get training ratings for this user\n",
    "        user_train_ratings = train_data.get(user_id, {})\n",
    "        \n",
    "        if not user_train_ratings:\n",
    "            continue\n",
    "        \n",
    "        # Initialize recommender\n",
    "        recommender = CollaborativeFilter(df)\n",
    "        recommender.update_user_ratings(user_train_ratings)\n",
    "        \n",
    "        user_predictions = []\n",
    "        user_actuals = []\n",
    "        \n",
    "        # Predict ratings for test movies\n",
    "        for movie_id, actual_rating in test_ratings.items():\n",
    "            predicted_rating = recommender.predict_rating(movie_id, train_data)\n",
    "            \n",
    "            user_predictions.append(predicted_rating)\n",
    "            user_actuals.append(actual_rating)\n",
    "            \n",
    "            all_predictions.append(predicted_rating)\n",
    "            all_actuals.append(actual_rating)\n",
    "        \n",
    "        # Calculate per-user metrics\n",
    "        if user_predictions:\n",
    "            user_mae = mean_absolute_error(user_actuals, user_predictions)\n",
    "            user_rmse = np.sqrt(mean_squared_error(user_actuals, user_predictions))\n",
    "            \n",
    "            user_results.append({\n",
    "                'user_id': user_id,\n",
    "                'mae': user_mae,\n",
    "                'rmse': user_rmse,\n",
    "                'num_predictions': len(user_predictions)\n",
    "            })\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_mae = mean_absolute_error(all_actuals, all_predictions)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_actuals, all_predictions))\n",
    "    \n",
    "    print(f\"\\n📊 Rating Prediction Results:\")\n",
    "    print(f\"• Overall MAE: {overall_mae:.3f}\")\n",
    "    print(f\"• Overall RMSE: {overall_rmse:.3f}\")\n",
    "    print(f\"• Total predictions: {len(all_predictions):,}\")\n",
    "    \n",
    "    return {\n",
    "        'mae': overall_mae,\n",
    "        'rmse': overall_rmse,\n",
    "        'predictions': all_predictions,\n",
    "        'actuals': all_actuals,\n",
    "        'user_results': user_results\n",
    "    }\n",
    "\n",
    "# Run rating prediction evaluation\n",
    "rating_results = evaluate_rating_prediction(train_data, test_data, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ranking Metrics (Precision@K, Recall@K, F1@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking_metrics(train_data, test_data, df, k_values=[5, 10], relevance_threshold=7):\n",
    "    \"\"\"\n",
    "    Evaluate ranking quality using Precision@K, Recall@K, F1@K, and NDCG@K.\n",
    "    \"\"\"\n",
    "    results = {k: {'precision': [], 'recall': [], 'f1': [], 'ndcg': []} for k in k_values}\n",
    "    \n",
    "    print(f\"🎯 Evaluating Ranking Metrics @K={k_values}...\")\n",
    "    \n",
    "    for i, (user_id, test_ratings) in enumerate(test_data.items()):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(test_data)} users\")\n",
    "            \n",
    "        user_train_ratings = train_data.get(user_id, {})\n",
    "        if not user_train_ratings:\n",
    "            continue\n",
    "            \n",
    "        # Get recommendations\n",
    "        try:\n",
    "            recommendations = get_recommendations(df, user_train_ratings, max(k_values), train_data)\n",
    "            \n",
    "            if recommendations.empty:\n",
    "                continue\n",
    "                \n",
    "            recommended_ids = recommendations['id'].astype(str).tolist()\n",
    "            \n",
    "            # Find relevant items in test set (high ratings)\n",
    "            relevant_items = set(movie_id for movie_id, rating in test_ratings.items() \n",
    "                               if rating >= relevance_threshold)\n",
    "            \n",
    "            if not relevant_items:\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics for each K\n",
    "            for k in k_values:\n",
    "                top_k_recs = set(recommended_ids[:k])\n",
    "                \n",
    "                # Precision@K\n",
    "                precision = len(top_k_recs & relevant_items) / k if k > 0 else 0\n",
    "                \n",
    "                # Recall@K\n",
    "                recall = len(top_k_recs & relevant_items) / len(relevant_items) if relevant_items else 0\n",
    "                \n",
    "                # F1@K\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                # NDCG@K (simplified)\n",
    "                dcg = 0\n",
    "                idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_items), k)))\n",
    "                \n",
    "                for i, movie_id in enumerate(recommended_ids[:k]):\n",
    "                    if movie_id in relevant_items:\n",
    "                        dcg += 1 / np.log2(i + 2)\n",
    "                \n",
    "                ndcg = dcg / idcg if idcg > 0 else 0\n",
    "                \n",
    "                results[k]['precision'].append(precision)\n",
    "                results[k]['recall'].append(recall)\n",
    "                results[k]['f1'].append(f1)\n",
    "                results[k]['ndcg'].append(ndcg)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages\n",
    "    summary = {}\n",
    "    for k in k_values:\n",
    "        summary[k] = {\n",
    "            'precision': np.mean(results[k]['precision']) if results[k]['precision'] else 0,\n",
    "            'recall': np.mean(results[k]['recall']) if results[k]['recall'] else 0,\n",
    "            'f1': np.mean(results[k]['f1']) if results[k]['f1'] else 0,\n",
    "            'ndcg': np.mean(results[k]['ndcg']) if results[k]['ndcg'] else 0,\n",
    "            'num_users': len(results[k]['precision'])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📊 Ranking Results @K={k}:\")\n",
    "        print(f\"• Precision@{k}: {summary[k]['precision']:.3f}\")\n",
    "        print(f\"• Recall@{k}: {summary[k]['recall']:.3f}\")\n",
    "        print(f\"• F1@{k}: {summary[k]['f1']:.3f}\")\n",
    "        print(f\"• NDCG@{k}: {summary[k]['ndcg']:.3f}\")\n",
    "        print(f\"• Evaluated users: {summary[k]['num_users']}\")\n",
    "    \n",
    "    return summary, results\n",
    "\n",
    "# Run ranking evaluation\n",
    "ranking_summary, ranking_details = evaluate_ranking_metrics(train_data, test_data, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Coverage and Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coverage_diversity(train_data, df, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Evaluate catalog coverage and recommendation diversity.\n",
    "    \"\"\"\n",
    "    all_recommendations = set()\n",
    "    genre_distributions = []\n",
    "    \n",
    "    print(\"🎨 Evaluating Coverage and Diversity...\")\n",
    "    \n",
    "    for i, (user_id, user_ratings) in enumerate(train_data.items()):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(train_data)} users\")\n",
    "            \n",
    "        try:\n",
    "            recommendations = get_recommendations(df, user_ratings, num_recommendations, train_data)\n",
    "            \n",
    "            if not recommendations.empty:\n",
    "                rec_ids = set(recommendations['id'].astype(str))\n",
    "                all_recommendations.update(rec_ids)\n",
    "                \n",
    "                # Analyze genre diversity\n",
    "                genres = []\n",
    "                for _, movie in recommendations.iterrows():\n",
    "                    movie_genres = str(movie['genre']).split(',')\n",
    "                    genres.extend([g.strip() for g in movie_genres if g.strip()])\n",
    "                \n",
    "                # Calculate genre distribution\n",
    "                unique_genres = len(set(genres))\n",
    "                genre_distributions.append(unique_genres)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    catalog_coverage = len(all_recommendations) / len(df)\n",
    "    avg_genre_diversity = np.mean(genre_distributions) if genre_distributions else 0\n",
    "    \n",
    "    print(f\"\\n📊 Coverage & Diversity Results:\")\n",
    "    print(f\"• Catalog Coverage: {catalog_coverage:.3f} ({len(all_recommendations):,}/{len(df):,} movies)\")\n",
    "    print(f\"• Avg Genre Diversity: {avg_genre_diversity:.2f} unique genres per user\")\n",
    "    print(f\"• Users analyzed: {len(genre_distributions)}\")\n",
    "    \n",
    "    return {\n",
    "        'catalog_coverage': catalog_coverage,\n",
    "        'unique_movies_recommended': len(all_recommendations),\n",
    "        'avg_genre_diversity': avg_genre_diversity,\n",
    "        'genre_distributions': genre_distributions\n",
    "    }\n",
    "\n",
    "# Run coverage and diversity evaluation\n",
    "coverage_results = evaluate_coverage_diversity(train_data, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. User Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_user_similarities(users_data, df, sample_size=10):\n",
    "    \"\"\"\n",
    "    Analyze user similarity patterns in the recommendation system.\n",
    "    \"\"\"\n",
    "    print(\"👥 Analyzing User Similarities...\")\n",
    "    \n",
    "    # Sample users for analysis\n",
    "    user_ids = list(users_data.keys())\n",
    "    if len(user_ids) > sample_size:\n",
    "        sample_users = np.random.choice(user_ids, sample_size, replace=False)\n",
    "    else:\n",
    "        sample_users = user_ids\n",
    "    \n",
    "    similarity_matrix = np.zeros((len(sample_users), len(sample_users)))\n",
    "    \n",
    "    for i, user1 in enumerate(sample_users):\n",
    "        recommender = CollaborativeFilter(df)\n",
    "        recommender.update_user_ratings(users_data[user1])\n",
    "        \n",
    "        # Build user-item matrix\n",
    "        current_user_id = \"current_user\"\n",
    "        all_users_with_current = users_data.copy()\n",
    "        all_users_with_current[current_user_id] = users_data[user1]\n",
    "        \n",
    "        try:\n",
    "            user_item_matrix = recommender._build_user_item_matrix(all_users_with_current)\n",
    "            \n",
    "            if not user_item_matrix.empty and current_user_id in user_item_matrix.index:\n",
    "                # Calculate similarities with other sample users\n",
    "                matrix_hash = hash(str(user_item_matrix.values.tobytes()))\n",
    "                similarities = recommender._calculate_user_similarity(matrix_hash, current_user_id)\n",
    "                \n",
    "                for j, user2 in enumerate(sample_users):\n",
    "                    if user2 in similarities.index:\n",
    "                        similarity_matrix[i, j] = similarities[user2]\n",
    "                        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_similarity = np.mean(similarity_matrix[similarity_matrix > 0])\n",
    "    max_similarity = np.max(similarity_matrix)\n",
    "    min_similarity = np.min(similarity_matrix[similarity_matrix > 0]) if np.any(similarity_matrix > 0) else 0\n",
    "    \n",
    "    print(f\"\\n📊 User Similarity Analysis:\")\n",
    "    print(f\"• Average Similarity: {avg_similarity:.3f}\")\n",
    "    print(f\"• Max Similarity: {max_similarity:.3f}\")\n",
    "    print(f\"• Min Similarity: {min_similarity:.3f}\")\n",
    "    print(f\"• Users analyzed: {len(sample_users)}\")\n",
    "    \n",
    "    return {\n",
    "        'similarity_matrix': similarity_matrix,\n",
    "        'user_ids': sample_users,\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'max_similarity': max_similarity,\n",
    "        'min_similarity': min_similarity\n",
    "    }\n",
    "\n",
    "# Run user similarity analysis\n",
    "similarity_results = analyze_user_similarities(users_data, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Movie Recommendation System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Rating Prediction Error Distribution\n",
    "errors = np.array(rating_results['predictions']) - np.array(rating_results['actuals'])\n",
    "axes[0, 0].hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', label='Perfect Prediction')\n",
    "axes[0, 0].set_title(f'Prediction Error Distribution\\nMAE: {rating_results[\"mae\"]:.3f}, RMSE: {rating_results[\"rmse\"]:.3f}')\n",
    "axes[0, 0].set_xlabel('Prediction Error (Predicted - Actual)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Ranking Metrics Comparison\n",
    "k_values = list(ranking_summary.keys())\n",
    "metrics = ['precision', 'recall', 'f1', 'ndcg']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    values = [ranking_summary[k][metric] for metric in metrics]\n",
    "    axes[0, 1].bar(x + i*width, values, width, label=f'K={k}', alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_title('Ranking Metrics @K')\n",
    "axes[0, 1].set_xlabel('Metrics')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_xticks(x + width/2)\n",
    "axes[0, 1].set_xticklabels([m.upper() for m in metrics])\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Genre Diversity Distribution\n",
    "if coverage_results['genre_distributions']:\n",
    "    axes[0, 2].hist(coverage_results['genre_distributions'], bins=15, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 2].axvline(coverage_results['avg_genre_diversity'], color='red', linestyle='--', \n",
    "                      label=f'Avg: {coverage_results[\"avg_genre_diversity\"]:.2f}')\n",
    "    axes[0, 2].set_title('Genre Diversity per User')\n",
    "    axes[0, 2].set_xlabel('Unique Genres in Recommendations')\n",
    "    axes[0, 2].set_ylabel('Number of Users')\n",
    "    axes[0, 2].legend()\n",
    "else:\n",
    "    axes[0, 2].text(0.5, 0.5, 'No Genre Data Available', ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "    axes[0, 2].set_title('Genre Diversity per User')\n",
    "\n",
    "# 4. User Similarity Heatmap\n",
    "if similarity_results['similarity_matrix'].size > 0:\n",
    "    im = axes[1, 0].imshow(similarity_results['similarity_matrix'], cmap='coolwarm', aspect='auto')\n",
    "    axes[1, 0].set_title('User Similarity Matrix')\n",
    "    axes[1, 0].set_xlabel('Users')\n",
    "    axes[1, 0].set_ylabel('Users')\n",
    "    plt.colorbar(im, ax=axes[1, 0], fraction=0.046, pad=0.04)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Similarity Matrix\\nNot Available', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('User Similarity Matrix')\n",
    "\n",
    "# 5. Prediction vs Actual Scatter Plot\n",
    "sample_size = min(1000, len(rating_results['predictions']))\n",
    "indices = np.random.choice(len(rating_results['predictions']), sample_size, replace=False)\n",
    "sample_pred = [rating_results['predictions'][i] for i in indices]\n",
    "sample_actual = [rating_results['actuals'][i] for i in indices]\n",
    "\n",
    "axes[1, 1].scatter(sample_actual, sample_pred, alpha=0.6, s=20)\n",
    "axes[1, 1].plot([1, 10], [1, 10], 'r--', label='Perfect Prediction')\n",
    "axes[1, 1].set_title('Predicted vs Actual Ratings')\n",
    "axes[1, 1].set_xlabel('Actual Rating')\n",
    "axes[1, 1].set_ylabel('Predicted Rating')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. System Performance Summary\n",
    "axes[1, 2].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "📊 SYSTEM PERFORMANCE SUMMARY\n",
    "\n",
    "Rating Prediction:\n",
    "• MAE: {rating_results['mae']:.3f}\n",
    "• RMSE: {rating_results['rmse']:.3f}\n",
    "• Predictions: {len(rating_results['predictions']):,}\n",
    "\n",
    "Ranking Quality @5:\n",
    "• Precision: {ranking_summary.get(5, {}).get('precision', 0):.3f}\n",
    "• Recall: {ranking_summary.get(5, {}).get('recall', 0):.3f}\n",
    "• F1-Score: {ranking_summary.get(5, {}).get('f1', 0):.3f}\n",
    "• NDCG: {ranking_summary.get(5, {}).get('ndcg', 0):.3f}\n",
    "\n",
    "Coverage & Diversity:\n",
    "• Catalog Coverage: {coverage_results['catalog_coverage']:.3f}\n",
    "• Avg Genre Diversity: {coverage_results['avg_genre_diversity']:.2f}\n",
    "• Unique Movies Recommended: {coverage_results['unique_movies_recommended']:,}\n",
    "\n",
    "User Similarity:\n",
    "• Average Similarity: {similarity_results['avg_similarity']:.3f}\n",
    "• Max Similarity: {similarity_results['max_similarity']:.3f}\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, \n",
    "               fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Evaluation Complete! Check the visualizations above for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Results Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "final_results = {\n",
    "    'evaluation_summary': {\n",
    "        'dataset_info': {\n",
    "            'total_movies': len(df),\n",
    "            'total_users': len(users_data),\n",
    "            'total_ratings': sum(len(ratings) for ratings in users_data.values()),\n",
    "            'sparsity': (1 - sum(len(ratings) for ratings in users_data.values()) / (len(users_data) * len(df))) * 100\n",
    "        },\n",
    "        'rating_prediction': {\n",
    "            'mae': rating_results['mae'],\n",
    "            'rmse': rating_results['rmse'],\n",
    "            'total_predictions': len(rating_results['predictions'])\n",
    "        },\n",
    "        'ranking_metrics': ranking_summary,\n",
    "        'coverage_diversity': {\n",
    "            'catalog_coverage': coverage_results['catalog_coverage'],\n",
    "            'unique_movies_recommended': coverage_results['unique_movies_recommended'],\n",
    "            'avg_genre_diversity': coverage_results['avg_genre_diversity']\n",
    "        },\n",
    "        'user_similarity': {\n",
    "            'avg_similarity': similarity_results['avg_similarity'],\n",
    "            'max_similarity': similarity_results['max_similarity'],\n",
    "            'min_similarity': similarity_results['min_similarity']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON file\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"💾 Results saved to 'evaluation_results.json'\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n🎯 FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {len(df):,} movies, {len(users_data)} users, {sum(len(r) for r in users_data.values()):,} ratings\")\n",
    "print(f\"Rating Prediction - MAE: {rating_results['mae']:.3f}, RMSE: {rating_results['rmse']:.3f}\")\n",
    "print(f\"Ranking @5 - Precision: {ranking_summary.get(5, {}).get('precision', 0):.3f}, Recall: {ranking_summary.get(5, {}).get('recall', 0):.3f}\")\n",
    "print(f\"Coverage: {coverage_results['catalog_coverage']:.3f} ({coverage_results['unique_movies_recommended']:,} movies)\")\n",
    "print(f\"Diversity: {coverage_results['avg_genre_diversity']:.2f} genres per user\")\n",
    "print(f\"User Similarity: Avg {similarity_results['avg_similarity']:.3f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}