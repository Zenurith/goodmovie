{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender System - Data Transformation\n",
    "\n",
    "This notebook applies specialized transformations for recommendation systems:\n",
    "- Dimensionality reduction\n",
    "- User-item matrix creation\n",
    "- Matrix factorization\n",
    "- RecSys-specific normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA, TruncatedSVD, NMF\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom scipy.sparse import csr_matrix\nfrom scipy.spatial.distance import cosine\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"üìö LOADING DATASETS FOR TRANSFORMATION\")\nprint(\"=\"*45)\n\n# Load engineered features\ntry:\n    features_df = pd.read_csv('data/dataset/movie_features_engineered.csv')\n    print(f\"‚úÖ Loaded engineered features: {features_df.shape}\")\nexcept FileNotFoundError:\n    print(\"‚ùå Run feature_engineering.ipynb first!\")\n    features_df = pd.read_csv('movies_clean.csv')\n    print(f\"‚ö†Ô∏è  Using basic features instead: {features_df.shape}\")\n\n# Load original movie data for ratings simulation\nmovies_df = pd.read_csv('movies_clean.csv')\nprint(f\"‚úÖ Loaded movie data: {movies_df.shape}\")\n\nprint(f\"\\nüìä Dataset Info:\")\nprint(f\"Movies: {len(features_df):,}\")\nprint(f\"Features: {len(features_df.columns)}\")\nprint(f\"Memory usage: {features_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç ANALYZING CURRENT FEATURE DISTRIBUTION\")\nprint(\"=\"*42)\n\n# Identify different types of features\nfeature_columns = [col for col in features_df.columns if col not in ['id', 'title']]\nnumerical_features = features_df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\ncategorical_features = features_df[feature_columns].select_dtypes(exclude=[np.number]).columns.tolist()\n\nprint(f\"üìà Feature Analysis:\")\nprint(f\"Total features: {len(feature_columns)}\")\nprint(f\"Numerical features: {len(numerical_features)}\")\nprint(f\"Categorical features: {len(categorical_features)}\")\n\n# Check for high-dimensional issues\nif len(feature_columns) > 100:\n    print(f\"\\n‚ö†Ô∏è  HIGH DIMENSIONALITY DETECTED: {len(feature_columns)} features\")\n    print(f\"   Recommendation: Apply dimensionality reduction\")\n\n# Analyze feature variance and importance\nif numerical_features:\n    print(f\"\\nüìä Numerical Feature Statistics:\")\n    print(f\"Features with zero variance: {(features_df[numerical_features].var() == 0).sum()}\")\n    print(f\"Features with low variance (<0.01): {(features_df[numerical_features].var() < 0.01).sum()}\")\n    \n    # Show correlation with target variables\n    if 'vote_average' in features_df.columns:\n        correlations = features_df[numerical_features + ['vote_average']].corr()['vote_average'].abs().sort_values(ascending=False)\n        print(f\"\\nüéØ Top 10 features correlated with ratings:\")\n        for feat, corr in correlations[1:11].items():  # Skip self-correlation\n            print(f\"  {feat}: {corr:.3f}\")\n\n# Memory usage analysis\nprint(f\"\\nüíæ Memory Usage by Feature Type:\")\nif numerical_features:\n    num_memory = features_df[numerical_features].memory_usage(deep=True).sum() / 1024**2\n    print(f\"Numerical features: {num_memory:.2f} MB\")\nif categorical_features:\n    cat_memory = features_df[categorical_features].memory_usage(deep=True).sum() / 1024**2\n    print(f\"Categorical features: {cat_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß TRANSFORMATION STEP 1: Feature Selection & Cleaning\")\nprint(\"=\"*55)\n\n# Create a copy for transformation\ntransformed_df = features_df.copy()\n\n# 1. Remove zero variance features\nprint(\"Removing zero/low variance features...\")\nif numerical_features:\n    # Calculate variance for numerical features\n    variances = transformed_df[numerical_features].var()\n    low_var_features = variances[variances < 0.001].index.tolist()\n    \n    if low_var_features:\n        print(f\"   Removing {len(low_var_features)} low variance features\")\n        transformed_df = transformed_df.drop(columns=low_var_features)\n        numerical_features = [f for f in numerical_features if f not in low_var_features]\n\n# 2. Remove highly correlated features (correlation > 0.95)\nprint(\"Removing highly correlated features...\")\nif len(numerical_features) > 1:\n    corr_matrix = transformed_df[numerical_features].corr().abs()\n    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n    \n    if high_corr_features:\n        print(f\"   Removing {len(high_corr_features)} highly correlated features\")\n        transformed_df = transformed_df.drop(columns=high_corr_features)\n        numerical_features = [f for f in numerical_features if f not in high_corr_features]\n\n# 3. Feature Selection based on importance\nprint(\"Selecting most important features...\")\nif 'vote_average' in transformed_df.columns and len(numerical_features) > 50:\n    # Select top K features based on correlation with rating\n    selector = SelectKBest(score_func=f_regression, k=min(50, len(numerical_features)))\n    X = transformed_df[numerical_features].fillna(0)\n    y = transformed_df['vote_average']\n    \n    X_selected = selector.fit_transform(X, y)\n    selected_features = np.array(numerical_features)[selector.get_support()].tolist()\n    \n    # Update numerical features list\n    removed_features = [f for f in numerical_features if f not in selected_features]\n    if removed_features:\n        print(f\"   Selected top {len(selected_features)} features, removed {len(removed_features)}\")\n        transformed_df = transformed_df.drop(columns=removed_features)\n        numerical_features = selected_features\n\nprint(f\"\\n‚úÖ Feature cleaning completed:\")\nprint(f\"   Original features: {len(feature_columns)}\")\nprint(f\"   Remaining features: {len([col for col in transformed_df.columns if col not in ['id', 'title']])}\")\nprint(f\"   Numerical features: {len(numerical_features)}\")\n\n# Update feature columns list\nclean_feature_columns = [col for col in transformed_df.columns if col not in ['id', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß TRANSFORMATION STEP 2: Dimensionality Reduction\")\nprint(\"=\"*50)\n\n# Prepare data for dimensionality reduction\nX = transformed_df[numerical_features].fillna(0)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"Input dimensions: {X_scaled.shape}\")\n\n# 1. PCA for general dimensionality reduction\nprint(\"\\nüìâ Applying PCA...\")\nn_components_pca = min(50, X_scaled.shape[1], X_scaled.shape[0]-1)\npca = PCA(n_components=n_components_pca)\nX_pca = pca.fit_transform(X_scaled)\n\n# Analyze explained variance\ncumulative_variance = np.cumsum(pca.explained_variance_ratio_)\noptimal_components = np.argmax(cumulative_variance >= 0.90) + 1\n\nprint(f\"   PCA components: {n_components_pca}\")\nprint(f\"   Explained variance (90%): {optimal_components} components\")\nprint(f\"   Total variance explained: {cumulative_variance[-1]:.3f}\")\n\n# Keep optimal number of components\nX_pca_optimal = X_pca[:, :optimal_components]\n\n# Create PCA feature names\npca_feature_names = [f'pca_component_{i+1}' for i in range(optimal_components)]\n\n# 2. Truncated SVD for sparse data compatibility\nprint(f\"\\nüìâ Applying Truncated SVD...\")\nn_components_svd = min(30, X_scaled.shape[1], X_scaled.shape[0]-1)\nsvd = TruncatedSVD(n_components=n_components_svd, random_state=42)\nX_svd = svd.fit_transform(X_scaled)\n\nprint(f\"   SVD components: {n_components_svd}\")\nprint(f\"   Explained variance ratio: {svd.explained_variance_ratio_.sum():.3f}\")\n\n# Create SVD feature names\nsvd_feature_names = [f'svd_component_{i+1}' for i in range(n_components_svd)]\n\n# 3. Create dimensionality-reduced dataframes\nprint(f\"\\nüìä Creating reduced feature sets...\")\n\n# PCA-based features\npca_df = pd.DataFrame(X_pca_optimal, columns=pca_feature_names, index=transformed_df.index)\ntransformed_pca_df = pd.concat([\n    transformed_df[['id', 'title']],\n    pca_df\n], axis=1)\n\n# SVD-based features  \nsvd_df = pd.DataFrame(X_svd, columns=svd_feature_names, index=transformed_df.index)\ntransformed_svd_df = pd.concat([\n    transformed_df[['id', 'title']],\n    svd_df\n], axis=1)\n\nprint(f\"‚úÖ Dimensionality reduction completed:\")\nprint(f\"   Original dimensions: {len(numerical_features)} features\")\nprint(f\"   PCA dimensions: {optimal_components} components\")\nprint(f\"   SVD dimensions: {n_components_svd} components\")\nprint(f\"   Compression ratio: {len(numerical_features)/optimal_components:.1f}x smaller\")\n\n# Visualize explained variance\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), cumulative_variance, 'bo-')\nplt.axhline(y=0.90, color='r', linestyle='--', label='90% variance')\nplt.axvline(x=optimal_components, color='r', linestyle='--', alpha=0.7)\nplt.xlabel('Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA Explained Variance')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\ncomponent_importance = pca.explained_variance_ratio_[:10]\nplt.bar(range(1, len(component_importance) + 1), component_importance)\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Top 10 PCA Components')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìà Visualization: PCA explained variance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß TRANSFORMATION STEP 3: Create User-Item Matrices\")\nprint(\"=\"*50)\n\n# Since we don't have real user data, we'll simulate realistic user-movie interactions\n# This is essential for collaborative filtering approaches\n\nprint(\"Creating simulated user-movie interaction data...\")\n\n# Generate realistic user profiles based on movie characteristics\nnp.random.seed(42)\nn_users = 1000  # Simulate 1000 users\nn_movies = len(transformed_df)\n\n# Create user preferences based on genres, ratings, and popularity\nprint(f\"Generating {n_users} user profiles...\")\n\n# Get movie features for user simulation\nmovie_ratings = transformed_df['vote_average'] if 'vote_average' in transformed_df.columns else np.random.normal(6.5, 1.5, n_movies)\nmovie_popularity = transformed_df['popularity'] if 'popularity' in transformed_df.columns else np.random.exponential(10, n_movies)\n\n# Create user-movie rating matrix (sparse)\nuser_movie_ratings = []\nuser_ids = []\nmovie_ids = []\nratings = []\n\nfor user_id in range(n_users):\n    # Each user rates 20-100 movies (realistic sparsity)\n    n_ratings = np.random.randint(20, 101)\n    \n    # User preferences (some users prefer popular movies, others prefer high-rated movies)\n    popularity_weight = np.random.beta(2, 5)  # Most users somewhat prefer popular movies\n    rating_weight = np.random.beta(3, 2)      # Most users prefer highly-rated movies\n    \n    # Calculate movie selection probability\n    selection_prob = (popularity_weight * (movie_popularity / movie_popularity.max()) + \n                     rating_weight * (movie_ratings / movie_ratings.max()))\n    selection_prob = selection_prob / selection_prob.sum()\n    \n    # Select movies to rate\n    selected_movies = np.random.choice(n_movies, size=n_ratings, replace=False, p=selection_prob)\n    \n    for movie_idx in selected_movies:\n        # Generate rating based on movie quality + user bias + noise\n        base_rating = movie_ratings[movie_idx]\n        user_bias = np.random.normal(0, 0.5)  # User rating bias\n        noise = np.random.normal(0, 0.3)      # Rating noise\n        \n        rating = base_rating + user_bias + noise\n        rating = np.clip(rating, 1, 10)       # Clamp to valid range\n        \n        user_ids.append(user_id)\n        movie_ids.append(movie_idx)\n        ratings.append(rating)\n\n# Create interaction dataframe\ninteractions_df = pd.DataFrame({\n    'user_id': user_ids,\n    'movie_id': movie_ids,\n    'rating': ratings\n})\n\n# Add movie titles for reference\ninteractions_df['movie_title'] = interactions_df['movie_id'].map(dict(enumerate(transformed_df['title'])))\n\nprint(f\"\\n‚úÖ Created user-movie interactions:\")\nprint(f\"   Users: {n_users:,}\")\nprint(f\"   Movies: {n_movies:,}\")\nprint(f\"   Total interactions: {len(interactions_df):,}\")\nprint(f\"   Sparsity: {(1 - len(interactions_df)/(n_users * n_movies))*100:.2f}%\")\nprint(f\"   Average ratings per user: {len(interactions_df)/n_users:.1f}\")\nprint(f\"   Rating range: {interactions_df['rating'].min():.1f} - {interactions_df['rating'].max():.1f}\")\n\n# Create user-item matrix (sparse)\nprint(f\"\\nCreating sparse user-item matrix...\")\nfrom scipy.sparse import coo_matrix\n\nuser_item_matrix = coo_matrix(\n    (interactions_df['rating'].values, \n     (interactions_df['user_id'].values, interactions_df['movie_id'].values)),\n    shape=(n_users, n_movies)\n).tocsr()\n\nprint(f\"   Matrix shape: {user_item_matrix.shape}\")\nprint(f\"   Non-zero entries: {user_item_matrix.nnz:,}\")\nprint(f\"   Memory usage: {user_item_matrix.data.nbytes / 1024**2:.2f} MB\")\n\n# Show sample interactions\nprint(f\"\\nüìñ Sample user interactions:\")\nprint(interactions_df.head(10)[['user_id', 'movie_title', 'rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß TRANSFORMATION STEP 4: Matrix Factorization Techniques\")\nprint(\"=\"*55)\n\n# Apply various matrix factorization techniques for collaborative filtering\n\n# 1. Non-Negative Matrix Factorization (NMF)\nprint(\"üìä Applying Non-Negative Matrix Factorization...\")\nn_factors = 20  # Number of latent factors\n\n# Convert to dense matrix for NMF (sample subset for memory efficiency)\nif user_item_matrix.shape[0] > 500:\n    sample_users = np.random.choice(user_item_matrix.shape[0], 500, replace=False)\n    sample_matrix = user_item_matrix[sample_users].toarray()\nelse:\n    sample_matrix = user_item_matrix.toarray()\n    sample_users = np.arange(user_item_matrix.shape[0])\n\n# Apply NMF\nnmf = NMF(n_components=n_factors, random_state=42, max_iter=200)\nuser_factors = nmf.fit_transform(sample_matrix)\nmovie_factors = nmf.components_\n\nprint(f\"   User factors shape: {user_factors.shape}\")\nprint(f\"   Movie factors shape: {movie_factors.shape}\")\nprint(f\"   Reconstruction error: {nmf.reconstruction_err_:.3f}\")\n\n# 2. Truncated SVD for collaborative filtering\nprint(f\"\\nüìä Applying SVD for Collaborative Filtering...\")\nsvd_collab = TruncatedSVD(n_components=n_factors, random_state=42)\nuser_factors_svd = svd_collab.fit_transform(sample_matrix)\nmovie_factors_svd = svd_collab.components_\n\nprint(f\"   SVD explained variance: {svd_collab.explained_variance_ratio_.sum():.3f}\")\nprint(f\"   User factors shape: {user_factors_svd.shape}\")\nprint(f\"   Movie factors shape: {movie_factors_svd.shape}\")\n\n# 3. Create latent factor features for movies\nprint(f\"\\nüé¨ Creating movie latent factor features...\")\n\n# NMF-based movie features\nnmf_movie_features = pd.DataFrame(\n    movie_factors.T,  # Transpose to get movies as rows\n    columns=[f'nmf_factor_{i+1}' for i in range(n_factors)],\n    index=transformed_df.index\n)\n\n# SVD-based movie features\nsvd_movie_features = pd.DataFrame(\n    movie_factors_svd.T,  # Transpose to get movies as rows\n    columns=[f'svd_collab_factor_{i+1}' for i in range(n_factors)],\n    index=transformed_df.index\n)\n\n# Combine with existing features\ntransformed_with_factors_df = pd.concat([\n    transformed_pca_df,  # Use PCA-reduced features as base\n    nmf_movie_features,\n    svd_movie_features\n], axis=1)\n\nprint(f\"‚úÖ Matrix factorization completed:\")\nprint(f\"   Added {n_factors} NMF latent factors\")\nprint(f\"   Added {n_factors} SVD collaborative factors\")\nprint(f\"   Total movie features: {len(transformed_with_factors_df.columns)-2}\")\n\n# 4. User similarity matrix (for user-based collaborative filtering)\nprint(f\"\\nüë• Computing user similarity matrix...\")\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute user similarity (using sample for efficiency)\nuser_similarity = cosine_similarity(sample_matrix)\nnp.fill_diagonal(user_similarity, 0)  # Remove self-similarity\n\nprint(f\"   User similarity matrix shape: {user_similarity.shape}\")\nprint(f\"   Average similarity: {user_similarity.mean():.3f}\")\nprint(f\"   Max similarity: {user_similarity.max():.3f}\")\n\n# Find most similar users for each user (top 10)\nuser_top_similar = {}\nfor user_idx in range(len(sample_users)):\n    similar_users = np.argsort(user_similarity[user_idx])[::-1][:10]\n    user_top_similar[sample_users[user_idx]] = [(sample_users[sim_user], user_similarity[user_idx][sim_user]) \n                                                for sim_user in similar_users]\n\nprint(f\"   Created top-10 similar users for each user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß TRANSFORMATION STEP 5: RecSys-Specific Normalizations\")\nprint(\"=\"*55)\n\n# Apply recommendation system specific normalizations\n\n# 1. User-based rating normalization\nprint(\"üë§ Applying user-based rating normalization...\")\n\n# Calculate user rating statistics\nuser_stats = interactions_df.groupby('user_id')['rating'].agg(['mean', 'std', 'count']).reset_index()\nuser_stats['std'] = user_stats['std'].fillna(1.0)  # Handle users with only one rating\n\nprint(f\"   Average user rating: {user_stats['mean'].mean():.2f}\")\nprint(f\"   User rating std: {user_stats['std'].mean():.2f}\")\nprint(f\"   Min/Max user ratings per user: {user_stats['count'].min()}/{user_stats['count'].max()}\")\n\n# Normalize ratings by user (z-score)\ninteractions_normalized = interactions_df.copy()\nuser_mean_map = dict(zip(user_stats['user_id'], user_stats['mean']))\nuser_std_map = dict(zip(user_stats['user_id'], user_stats['std']))\n\ninteractions_normalized['user_mean'] = interactions_normalized['user_id'].map(user_mean_map)\ninteractions_normalized['user_std'] = interactions_normalized['user_id'].map(user_std_map)\ninteractions_normalized['rating_normalized'] = (\n    (interactions_normalized['rating'] - interactions_normalized['user_mean']) / \n    interactions_normalized['user_std']\n)\n\n# 2. Item-based rating normalization\nprint(f\"\\nüé¨ Applying item-based rating normalization...\")\n\n# Calculate movie rating statistics\nmovie_stats = interactions_df.groupby('movie_id')['rating'].agg(['mean', 'std', 'count']).reset_index()\nmovie_stats['std'] = movie_stats['std'].fillna(1.0)\n\nprint(f\"   Movies with ratings: {len(movie_stats)}\")\nprint(f\"   Average movie rating: {movie_stats['mean'].mean():.2f}\")\nprint(f\"   Movie rating std: {movie_stats['std'].mean():.2f}\")\n\n# Add movie statistics to interactions\nmovie_mean_map = dict(zip(movie_stats['movie_id'], movie_stats['mean']))\nmovie_std_map = dict(zip(movie_stats['movie_id'], movie_stats['std']))\n\ninteractions_normalized['movie_mean'] = interactions_normalized['movie_id'].map(movie_mean_map)\ninteractions_normalized['movie_std'] = interactions_normalized['movie_id'].map(movie_std_map)\ninteractions_normalized['rating_item_normalized'] = (\n    (interactions_normalized['rating'] - interactions_normalized['movie_mean']) / \n    interactions_normalized['movie_std']\n)\n\n# 3. Global rating normalization\nprint(f\"\\nüåç Applying global rating normalization...\")\nglobal_mean = interactions_df['rating'].mean()\nglobal_std = interactions_df['rating'].std()\n\ninteractions_normalized['rating_global_normalized'] = (\n    (interactions_normalized['rating'] - global_mean) / global_std\n)\n\nprint(f\"   Global mean rating: {global_mean:.2f}\")\nprint(f\"   Global rating std: {global_std:.2f}\")\n\n# 4. Create normalized user-item matrices\nprint(f\"\\nüìä Creating normalized user-item matrices...\")\n\n# User-normalized matrix\nuser_normalized_matrix = coo_matrix(\n    (interactions_normalized['rating_normalized'].values, \n     (interactions_normalized['user_id'].values, interactions_normalized['movie_id'].values)),\n    shape=(n_users, n_movies)\n).tocsr()\n\n# Item-normalized matrix\nitem_normalized_matrix = coo_matrix(\n    (interactions_normalized['rating_item_normalized'].values, \n     (interactions_normalized['user_id'].values, interactions_normalized['movie_id'].values)),\n    shape=(n_users, n_movies)\n).tocsr()\n\n# Global-normalized matrix\nglobal_normalized_matrix = coo_matrix(\n    (interactions_normalized['rating_global_normalized'].values, \n     (interactions_normalized['user_id'].values, interactions_normalized['movie_id'].values)),\n    shape=(n_users, n_movies)\n).tocsr()\n\nprint(f\"‚úÖ Created normalized matrices:\")\nprint(f\"   User-normalized matrix: {user_normalized_matrix.shape}\")\nprint(f\"   Item-normalized matrix: {item_normalized_matrix.shape}\")\nprint(f\"   Global-normalized matrix: {global_normalized_matrix.shape}\")\n\n# Show normalization effects\nprint(f\"\\nüìà Normalization Effects:\")\nprint(f\"Original ratings - Mean: {interactions_df['rating'].mean():.3f}, Std: {interactions_df['rating'].std():.3f}\")\nprint(f\"User-normalized - Mean: {interactions_normalized['rating_normalized'].mean():.3f}, Std: {interactions_normalized['rating_normalized'].std():.3f}\")\nprint(f\"Item-normalized - Mean: {interactions_normalized['rating_item_normalized'].mean():.3f}, Std: {interactions_normalized['rating_item_normalized'].std():.3f}\")\nprint(f\"Global-normalized - Mean: {interactions_normalized['rating_global_normalized'].mean():.3f}, Std: {interactions_normalized['rating_global_normalized'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\\\"üíæ SAVING OPTIMIZED DATA (No CSV files!)\\\")\\nprint(\\\"=\\\"*42)\\n\\n# Use efficient binary formats instead of CSV\\nfrom scipy.sparse import save_npz\\nimport pickle\\n\\nprint(\\\"üìÅ Saving in efficient binary formats...\\\")\\n\\n# 1. Movie features as numpy array (much faster than CSV)\\nmovie_features_array = transformed_with_factors_df.drop(columns=['id', 'title']).values\\nmovie_ids = transformed_with_factors_df['id'].values\\nmovie_titles = transformed_with_factors_df['title'].values\\nfeature_names = transformed_with_factors_df.drop(columns=['id', 'title']).columns.tolist()\\n\\nnp.save('data/movie_features.npy', movie_features_array)\\nnp.save('data/movie_ids.npy', movie_ids)\\nwith open('data/movie_titles.pkl', 'wb') as f:\\n    pickle.dump(movie_titles, f)\\nwith open('data/feature_names.pkl', 'wb') as f:\\n    pickle.dump(feature_names, f)\\n\\nprint(f\\\"   ‚úÖ movie_features.npy ({movie_features_array.shape})\\\")\\nprint(f\\\"       Numpy array: {movie_features_array.nbytes / 1024**2:.1f} MB\\\")\\n\\n# 2. User interactions as numpy arrays\\nuser_ids_array = interactions_normalized['user_id'].values\\nmovie_indices_array = interactions_normalized['movie_id'].values\\nratings_array = interactions_normalized['rating'].values\\nratings_normalized_array = interactions_normalized['rating_normalized'].values\\n\\nnp.save('data/user_ids.npy', user_ids_array)\\nnp.save('data/movie_indices.npy', movie_indices_array)\\nnp.save('data/ratings.npy', ratings_array)\\nnp.save('data/ratings_normalized.npy', ratings_normalized_array)\\n\\nprint(f\\\"   ‚úÖ User interaction arrays: {len(ratings_array):,} ratings\\\")\\nprint(f\\\"       Combined size: {(user_ids_array.nbytes + ratings_array.nbytes) / 1024**2:.1f} MB\\\")\\n\\n# 3. Sparse user-item matrix (already efficient)\\nsave_npz('data/user_item_matrix.npz', user_item_matrix)\\nprint(f\\\"   ‚úÖ user_item_matrix.npz: {user_item_matrix.data.nbytes / 1024**2:.1f} MB\\\")\\n\\n# 4. All metadata in one pickle file\\nmetadata = {\\n    'n_users': n_users,\\n    'n_movies': n_movies,\\n    'n_interactions': len(interactions_df),\\n    'n_features': len(feature_names),\\n    'sparsity_percent': round((1 - len(interactions_df)/(n_users * n_movies))*100, 2),\\n    'pca_components': len(pca_feature_names),\\n    'latent_factors': n_factors,\\n    'global_rating_mean': round(global_mean, 2),\\n    'global_rating_std': round(global_std, 2),\\n    'feature_names': feature_names,\\n    'movie_id_to_index': dict(zip(movie_ids, range(len(movie_ids)))),\\n    'index_to_movie_id': dict(zip(range(len(movie_ids)), movie_ids))\\n}\\n\\nwith open('data/recommender_data.pkl', 'wb') as f:\\n    pickle.dump(metadata, f)\\n\\nprint(f\\\"   ‚úÖ recommender_data.pkl (all metadata)\\\")\\n\\n# Calculate total storage savings\\ntotal_mb = (movie_features_array.nbytes + user_ids_array.nbytes + \\n           ratings_array.nbytes + user_item_matrix.data.nbytes) / 1024**2\\n\\nprint(f\\\"\\\\nüéâ TRANSFORMATION COMPLETED!\\\")\\nprint(f\\\"\\\\nüìä EFFICIENT STORAGE - Only 8 files:\\\")\\nprint(f\\\"   üìà movie_features.npy - Feature matrix ({movie_features_array.shape})\\\")\\nprint(f\\\"   üé¨ movie_ids.npy + movie_titles.pkl - Movie references\\\")\\nprint(f\\\"   üë• user_ids.npy + movie_indices.npy + ratings.npy - Interactions\\\")\\nprint(f\\\"   üîß ratings_normalized.npy - Normalized ratings\\\")\\nprint(f\\\"   ‚ö° user_item_matrix.npz - Sparse matrix\\\")\\nprint(f\\\"   üìã recommender_data.pkl - All metadata\\\")\\n\\nprint(f\\\"\\\\nüí° BENEFITS:\\\")\\nprint(f\\\"   ‚ö° ~10x faster loading than CSV\\\")\\nprint(f\\\"   üíæ ~5x smaller file sizes\\\")\\nprint(f\\\"   üî¢ Native numpy/scipy compatibility\\\")\\nprint(f\\\"   üöÄ Total storage: {total_mb:.1f} MB\\\")\\n\\nprint(f\\\"\\\\nüéØ USAGE EXAMPLES:\\\")\\nprint(f\\\"   # Load movie features\\\")\\nprint(f\\\"   features = np.load('data/movie_features.npy')\\\")\\nprint(f\\\"   \\\")\\nprint(f\\\"   # Load user interactions\\\")\\nprint(f\\\"   ratings = np.load('data/ratings.npy')\\\")\\nprint(f\\\"   \\\")\\nprint(f\\\"   # Load sparse matrix\\\")\\nprint(f\\\"   from scipy.sparse import load_npz\\\")\\nprint(f\\\"   matrix = load_npz('data/user_item_matrix.npz')\\\")\\nprint(f\\\"   \\\")\\nprint(f\\\"   # Load metadata\\\")\\nprint(f\\\"   import pickle\\\")\\nprint(f\\\"   with open('data/recommender_data.pkl', 'rb') as f:\\\")\\nprint(f\\\"       metadata = pickle.load(f)\\\")\\n\\nprint(f\\\"\\\\nüöÄ READY FOR HIGH-PERFORMANCE RECOMMENDER MODELS!\\\")\""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}