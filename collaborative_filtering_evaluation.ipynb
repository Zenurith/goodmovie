{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Evaluation Using Cosine Similarity\n",
    "\n",
    "This notebook evaluates the collaborative filtering algorithm in `collaborative_filtering.py` using only cosine similarity-based methods. We'll test accuracy, performance, and recommendation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our collaborative filtering module\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from algorithm.collaborative_filtering import CollaborativeFilter, get_recommendations\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie dataset\n",
    "print(\"üì• Loading movie dataset...\")\n",
    "movies_df = pd.read_csv('dataset/movies.csv')\n",
    "\n",
    "# Display basic dataset info\n",
    "print(f\"Dataset shape: {movies_df.shape}\")\n",
    "print(f\"Columns: {list(movies_df.columns)}\")\n",
    "print(\"\\nFirst 5 movies:\")\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user ratings data\n",
    "print(\"üë• Loading user ratings data...\")\n",
    "import json\n",
    "\n",
    "try:\n",
    "    with open('users_data.json', 'r') as f:\n",
    "        users_data = json.load(f)\n",
    "    \n",
    "    # Extract ratings from users data\n",
    "    all_user_ratings = {}\n",
    "    if 'users' in users_data:\n",
    "        for user_id, user_info in users_data['users'].items():\n",
    "            if 'ratings' in user_info and user_info['ratings']:\n",
    "                all_user_ratings[user_id] = user_info['ratings']\n",
    "    \n",
    "    print(f\"Total users with ratings: {len(all_user_ratings)}\")\n",
    "    if all_user_ratings:\n",
    "        total_ratings = sum(len(ratings) for ratings in all_user_ratings.values())\n",
    "        print(f\"Total ratings: {total_ratings}\")\n",
    "        avg_ratings_per_user = total_ratings / len(all_user_ratings)\n",
    "        print(f\"Average ratings per user: {avg_ratings_per_user:.1f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No user ratings found, creating synthetic data for testing...\")\n",
    "        all_user_ratings = create_synthetic_ratings(movies_df)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è users_data.json not found, creating synthetic data for testing...\")\n",
    "    all_user_ratings = create_synthetic_ratings(movies_df)\n",
    "\n",
    "def create_synthetic_ratings(movies_df, n_users=50, n_ratings_per_user=20):\n",
    "    \"\"\"Create synthetic user ratings for testing purposes\"\"\"\n",
    "    synthetic_ratings = {}\n",
    "    movie_ids = movies_df['id'].tolist()[:1000]  # Use first 1000 movies\n",
    "    \n",
    "    for user_id in range(n_users):\n",
    "        # Randomly select movies for this user\n",
    "        user_movies = np.random.choice(movie_ids, n_ratings_per_user, replace=False)\n",
    "        user_ratings = {}\n",
    "        \n",
    "        for movie_id in user_movies:\n",
    "            # Generate ratings with some bias towards higher ratings\n",
    "            rating = np.random.choice([5,6,7,8,9,10], p=[0.05,0.1,0.2,0.3,0.25,0.1])\n",
    "            user_ratings[str(movie_id)] = rating\n",
    "            \n",
    "        synthetic_ratings[f\"user_{user_id}\"] = user_ratings\n",
    "    \n",
    "    return synthetic_ratings\n",
    "\n",
    "# If we created synthetic data, use it\n",
    "if not all_user_ratings:\n",
    "    all_user_ratings = create_synthetic_ratings(movies_df)\n",
    "    print(f\"‚úÖ Created synthetic data: {len(all_user_ratings)} users\")\n",
    "\n",
    "print(\"\\nSample user ratings:\")\n",
    "sample_user = list(all_user_ratings.keys())[0]\n",
    "print(f\"User '{sample_user}' has {len(all_user_ratings[sample_user])} ratings\")\n",
    "print(f\"Sample ratings: {dict(list(all_user_ratings[sample_user].items())[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cosine Similarity Implementation & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityEvaluator:\n",
    "    \"\"\"Pure cosine similarity-based collaborative filtering evaluator\"\"\"\n",
    "    \n",
    "    def __init__(self, movies_df, user_ratings_dict):\n",
    "        self.movies_df = movies_df\n",
    "        self.user_ratings_dict = user_ratings_dict\n",
    "        self.user_item_matrix = None\n",
    "        self.similarity_matrix = None\n",
    "        self._build_user_item_matrix()\n",
    "        \n",
    "    def _build_user_item_matrix(self):\n",
    "        \"\"\"Build user-item rating matrix\"\"\"\n",
    "        print(\"üî® Building user-item matrix...\")\n",
    "        \n",
    "        # Get all unique movie IDs from ratings\n",
    "        all_movie_ids = set()\n",
    "        for user_ratings in self.user_ratings_dict.values():\n",
    "            all_movie_ids.update(user_ratings.keys())\n",
    "        \n",
    "        all_movie_ids = sorted(list(all_movie_ids))\n",
    "        user_ids = list(self.user_ratings_dict.keys())\n",
    "        \n",
    "        print(f\"Matrix dimensions: {len(user_ids)} users √ó {len(all_movie_ids)} movies\")\n",
    "        \n",
    "        # Create matrix\n",
    "        matrix_data = np.zeros((len(user_ids), len(all_movie_ids)))\n",
    "        \n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            user_ratings = self.user_ratings_dict[user_id]\n",
    "            for j, movie_id in enumerate(all_movie_ids):\n",
    "                if movie_id in user_ratings:\n",
    "                    matrix_data[i, j] = user_ratings[movie_id]\n",
    "        \n",
    "        self.user_item_matrix = pd.DataFrame(\n",
    "            matrix_data, \n",
    "            index=user_ids, \n",
    "            columns=all_movie_ids\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Matrix built successfully!\")\n",
    "        print(f\"Sparsity: {(matrix_data == 0).sum() / matrix_data.size * 100:.1f}%\")\n",
    "        \n",
    "    def calculate_user_similarity(self, target_user_id, method='cosine'):\n",
    "        \"\"\"Calculate similarity between target user and all other users\"\"\"\n",
    "        if target_user_id not in self.user_item_matrix.index:\n",
    "            return pd.Series(dtype=float)\n",
    "            \n",
    "        target_ratings = self.user_item_matrix.loc[target_user_id].values.reshape(1, -1)\n",
    "        all_ratings = self.user_item_matrix.values\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(target_ratings, all_ratings)[0]\n",
    "        \n",
    "        return pd.Series(similarities, index=self.user_item_matrix.index)\n",
    "    \n",
    "    def predict_rating(self, user_id, movie_id, k=10):\n",
    "        \"\"\"Predict rating using k most similar users\"\"\"\n",
    "        if user_id not in self.user_item_matrix.index:\n",
    "            return self.movies_df[self.movies_df['id'].astype(str) == movie_id]['vote_average'].iloc[0] if len(self.movies_df[self.movies_df['id'].astype(str) == movie_id]) > 0 else 5.0\n",
    "        \n",
    "        # Get user similarities\n",
    "        similarities = self.calculate_user_similarity(user_id)\n",
    "        \n",
    "        # Remove target user and get top k similar users\n",
    "        similarities = similarities.drop(user_id, errors='ignore')\n",
    "        top_similar = similarities.nlargest(k)\n",
    "        \n",
    "        # Get ratings for the movie from similar users\n",
    "        weighted_sum = 0\n",
    "        similarity_sum = 0\n",
    "        \n",
    "        for similar_user, similarity_score in top_similar.items():\n",
    "            if similarity_score > 0.1:  # Minimum similarity threshold\n",
    "                if movie_id in self.user_ratings_dict[similar_user]:\n",
    "                    rating = self.user_ratings_dict[similar_user][movie_id]\n",
    "                    weighted_sum += similarity_score * rating\n",
    "                    similarity_sum += similarity_score\n",
    "        \n",
    "        if similarity_sum > 0:\n",
    "            predicted_rating = weighted_sum / similarity_sum\n",
    "            return max(1, min(10, predicted_rating))\n",
    "        else:\n",
    "            # Fallback to movie's average rating\n",
    "            movie_avg = self.movies_df[self.movies_df['id'].astype(str) == movie_id]['vote_average']\n",
    "            return movie_avg.iloc[0] if len(movie_avg) > 0 else 5.0\n",
    "    \n",
    "    def recommend_movies(self, user_id, n_recommendations=10, k=10):\n",
    "        \"\"\"Recommend movies using cosine similarity\"\"\"\n",
    "        if user_id not in self.user_ratings_dict:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        user_ratings = self.user_ratings_dict[user_id]\n",
    "        rated_movies = set(user_ratings.keys())\n",
    "        \n",
    "        # Get all movies not rated by the user\n",
    "        all_movies = set(self.movies_df['id'].astype(str))\n",
    "        unrated_movies = all_movies - rated_movies\n",
    "        \n",
    "        # Predict ratings for unrated movies\n",
    "        predictions = []\n",
    "        for movie_id in list(unrated_movies)[:500]:  # Limit for performance\n",
    "            predicted_rating = self.predict_rating(user_id, movie_id, k)\n",
    "            predictions.append((movie_id, predicted_rating))\n",
    "        \n",
    "        # Sort by predicted rating\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get movie details for top recommendations\n",
    "        recommended_movies = []\n",
    "        for movie_id, pred_rating in predictions[:n_recommendations]:\n",
    "            movie_info = self.movies_df[self.movies_df['id'].astype(str) == movie_id]\n",
    "            if not movie_info.empty:\n",
    "                movie_dict = movie_info.iloc[0].to_dict()\n",
    "                movie_dict['predicted_rating'] = pred_rating\n",
    "                recommended_movies.append(movie_dict)\n",
    "        \n",
    "        return pd.DataFrame(recommended_movies)\n",
    "\n",
    "# Initialize evaluator\n",
    "print(\"üöÄ Initializing Cosine Similarity Evaluator...\")\n",
    "evaluator = CosineSimilarityEvaluator(movies_df, all_user_ratings)\n",
    "print(\"‚úÖ Evaluator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction_accuracy(evaluator, test_size=0.2, k_values=[5, 10, 20]):\n",
    "    \"\"\"Evaluate prediction accuracy using train-test split\"\"\"\n",
    "    print(\"üéØ Evaluating Prediction Accuracy...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Select users with enough ratings for meaningful evaluation\n",
    "    suitable_users = [\n",
    "        user_id for user_id, ratings in all_user_ratings.items() \n",
    "        if len(ratings) >= 10\n",
    "    ][:20]  # Limit to 20 users for performance\n",
    "    \n",
    "    print(f\"Testing with {len(suitable_users)} users...\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nüìä Testing with k={k} similar users...\")\n",
    "        \n",
    "        mae_scores = []\n",
    "        rmse_scores = []\n",
    "        \n",
    "        for i, user_id in enumerate(suitable_users):\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Processing user {i+1}/{len(suitable_users)}...\")\n",
    "            \n",
    "            user_ratings = all_user_ratings[user_id]\n",
    "            \n",
    "            # Split ratings into train and test\n",
    "            items = list(user_ratings.items())\n",
    "            train_items, test_items = train_test_split(items, test_size=test_size, random_state=42)\n",
    "            \n",
    "            train_ratings = dict(train_items)\n",
    "            test_ratings = dict(test_items)\n",
    "            \n",
    "            # Create temporary evaluator with train data only\n",
    "            temp_user_ratings = all_user_ratings.copy()\n",
    "            temp_user_ratings[user_id] = train_ratings\n",
    "            temp_evaluator = CosineSimilarityEvaluator(movies_df, temp_user_ratings)\n",
    "            \n",
    "            # Predict ratings for test movies\n",
    "            predictions = []\n",
    "            actuals = []\n",
    "            \n",
    "            for movie_id, actual_rating in test_items:\n",
    "                predicted = temp_evaluator.predict_rating(user_id, movie_id, k)\n",
    "                predictions.append(predicted)\n",
    "                actuals.append(actual_rating)\n",
    "            \n",
    "            if predictions and actuals:\n",
    "                mae = mean_absolute_error(actuals, predictions)\n",
    "                rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "                mae_scores.append(mae)\n",
    "                rmse_scores.append(rmse)\n",
    "        \n",
    "        avg_mae = np.mean(mae_scores) if mae_scores else float('inf')\n",
    "        avg_rmse = np.mean(rmse_scores) if rmse_scores else float('inf')\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'MAE': avg_mae,\n",
    "            'RMSE': avg_rmse,\n",
    "            'num_users_tested': len(mae_scores)\n",
    "        })\n",
    "        \n",
    "        print(f\"  MAE: {avg_mae:.3f}, RMSE: {avg_rmse:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run prediction accuracy evaluation\n",
    "accuracy_results = evaluate_prediction_accuracy(evaluator)\n",
    "print(\"\\nüìà Prediction Accuracy Results:\")\n",
    "accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommendation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recommendation_quality(evaluator, sample_users=5):\n",
    "    \"\"\"Analyze quality and diversity of recommendations\"\"\"\n",
    "    print(\"üé¨ Analyzing Recommendation Quality...\")\n",
    "    \n",
    "    sample_user_ids = list(all_user_ratings.keys())[:sample_users]\n",
    "    \n",
    "    quality_metrics = []\n",
    "    \n",
    "    for user_id in sample_user_ids:\n",
    "        print(f\"\\nüë§ User: {user_id}\")\n",
    "        \n",
    "        # Get user's actual preferences\n",
    "        user_ratings = all_user_ratings[user_id]\n",
    "        avg_user_rating = np.mean(list(user_ratings.values()))\n",
    "        \n",
    "        print(f\"  User's average rating: {avg_user_rating:.1f}\")\n",
    "        print(f\"  Number of rated movies: {len(user_ratings)}\")\n",
    "        \n",
    "        # Get top rated movies by user\n",
    "        top_rated = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(\"  Top rated movies:\")\n",
    "        for movie_id, rating in top_rated:\n",
    "            movie_title = movies_df[movies_df['id'].astype(str) == movie_id]['title']\n",
    "            title = movie_title.iloc[0] if len(movie_title) > 0 else f\"Movie {movie_id}\"\n",
    "            print(f\"    - {title}: {rating}/10\")\n",
    "        \n",
    "        # Get recommendations\n",
    "        recommendations = evaluator.recommend_movies(user_id, n_recommendations=10)\n",
    "        \n",
    "        if not recommendations.empty:\n",
    "            print(\"  \\nüéØ Top 5 Recommendations:\")\n",
    "            for i, (_, movie) in enumerate(recommendations.head(5).iterrows()):\n",
    "                print(f\"    {i+1}. {movie['title']} (Predicted: {movie['predicted_rating']:.1f}, Actual: {movie['vote_average']:.1f})\")\n",
    "            \n",
    "            # Calculate quality metrics\n",
    "            avg_predicted_rating = recommendations['predicted_rating'].mean()\n",
    "            avg_actual_rating = recommendations['vote_average'].mean()\n",
    "            \n",
    "            # Genre diversity\n",
    "            all_genres = set()\n",
    "            for genres_str in recommendations['genre'].fillna(''):\n",
    "                genres = [g.strip() for g in str(genres_str).split(',')]\n",
    "                all_genres.update(genres)\n",
    "            genre_diversity = len(all_genres)\n",
    "            \n",
    "            # Popularity distribution\n",
    "            popularity_std = recommendations['popularity'].std()\n",
    "            \n",
    "            quality_metrics.append({\n",
    "                'user_id': user_id,\n",
    "                'user_avg_rating': avg_user_rating,\n",
    "                'num_user_ratings': len(user_ratings),\n",
    "                'avg_predicted_rating': avg_predicted_rating,\n",
    "                'avg_actual_rating': avg_actual_rating,\n",
    "                'genre_diversity': genre_diversity,\n",
    "                'popularity_diversity': popularity_std\n",
    "            })\n",
    "            \n",
    "            print(f\"  üìä Quality Metrics:\")\n",
    "            print(f\"    Average predicted rating: {avg_predicted_rating:.2f}\")\n",
    "            print(f\"    Average actual rating: {avg_actual_rating:.2f}\")\n",
    "            print(f\"    Genre diversity: {genre_diversity} unique genres\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è No recommendations generated\")\n",
    "    \n",
    "    return pd.DataFrame(quality_metrics)\n",
    "\n",
    "# Run recommendation quality analysis\n",
    "quality_results = analyze_recommendation_quality(evaluator)\n",
    "print(\"\\nüìä Quality Analysis Summary:\")\n",
    "quality_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(evaluator):\n",
    "    \"\"\"Benchmark performance of different components\"\"\"\n",
    "    print(\"‚ö° Benchmarking Performance...\")\n",
    "    \n",
    "    sample_users = list(all_user_ratings.keys())[:10]\n",
    "    performance_results = []\n",
    "    \n",
    "    # Test similarity calculation\n",
    "    print(\"\\nüîç Testing similarity calculation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for user_id in sample_users:\n",
    "        similarities = evaluator.calculate_user_similarity(user_id)\n",
    "    \n",
    "    similarity_time = time.time() - start_time\n",
    "    print(f\"  Time for {len(sample_users)} similarity calculations: {similarity_time:.3f}s\")\n",
    "    print(f\"  Average time per similarity calculation: {similarity_time/len(sample_users):.3f}s\")\n",
    "    \n",
    "    # Test rating prediction\n",
    "    print(\"\\nüéØ Testing rating prediction...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_movies = list(movies_df['id'].astype(str))[:50]  # First 50 movies\n",
    "    predictions_made = 0\n",
    "    \n",
    "    for user_id in sample_users[:5]:  # Test with 5 users\n",
    "        for movie_id in test_movies[:10]:  # Test with 10 movies each\n",
    "            evaluator.predict_rating(user_id, movie_id)\n",
    "            predictions_made += 1\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    print(f\"  Time for {predictions_made} predictions: {prediction_time:.3f}s\")\n",
    "    print(f\"  Average time per prediction: {prediction_time/predictions_made:.4f}s\")\n",
    "    \n",
    "    # Test full recommendation generation\n",
    "    print(\"\\nüé¨ Testing recommendation generation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for user_id in sample_users[:3]:  # Test with 3 users\n",
    "        recommendations = evaluator.recommend_movies(user_id, n_recommendations=10)\n",
    "    \n",
    "    recommendation_time = time.time() - start_time\n",
    "    print(f\"  Time for 3 full recommendation sets: {recommendation_time:.3f}s\")\n",
    "    print(f\"  Average time per recommendation set: {recommendation_time/3:.3f}s\")\n",
    "    \n",
    "    # Memory usage analysis\n",
    "    print(\"\\nüíæ Memory Usage Analysis:\")\n",
    "    matrix_size = evaluator.user_item_matrix.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"  User-item matrix size: {matrix_size:.1f} MB\")\n",
    "    print(f\"  Matrix dimensions: {evaluator.user_item_matrix.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'similarity_time_per_user': similarity_time/len(sample_users),\n",
    "        'prediction_time_per_rating': prediction_time/predictions_made,\n",
    "        'recommendation_time_per_user': recommendation_time/3,\n",
    "        'matrix_memory_mb': matrix_size\n",
    "    }\n",
    "\n",
    "# Run performance benchmark\n",
    "perf_results = benchmark_performance(evaluator)\n",
    "print(\"\\n‚ö° Performance Summary:\")\n",
    "for metric, value in perf_results.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Original Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_original_algorithm():\n",
    "    \"\"\"Compare pure cosine similarity with original hybrid algorithm\"\"\"\n",
    "    print(\"üîÑ Comparing with Original Algorithm...\")\n",
    "    \n",
    "    # Test with sample users\n",
    "    test_users = list(all_user_ratings.keys())[:3]\n",
    "    comparison_results = []\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        user_ratings = all_user_ratings[user_id]\n",
    "        \n",
    "        print(f\"\\nüë§ Testing User: {user_id}\")\n",
    "        print(f\"  User has {len(user_ratings)} ratings\")\n",
    "        \n",
    "        # Get recommendations from pure cosine similarity\n",
    "        start_time = time.time()\n",
    "        cosine_recs = evaluator.recommend_movies(user_id, n_recommendations=10)\n",
    "        cosine_time = time.time() - start_time\n",
    "        \n",
    "        # Get recommendations from original algorithm\n",
    "        start_time = time.time()\n",
    "        original_recs = get_recommendations(movies_df, user_ratings, n_recommendations=10)\n",
    "        original_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  üïê Cosine Similarity Time: {cosine_time:.3f}s\")\n",
    "        print(f\"  üïë Original Algorithm Time: {original_time:.3f}s\")\n",
    "        \n",
    "        # Compare recommendations\n",
    "        cosine_titles = set(cosine_recs['title'].tolist() if not cosine_recs.empty else [])\n",
    "        original_titles = set(original_recs['title'].tolist() if not original_recs.empty else [])\n",
    "        \n",
    "        overlap = len(cosine_titles.intersection(original_titles))\n",
    "        \n",
    "        print(f\"  üìä Recommendation Overlap: {overlap}/10 movies\")\n",
    "        \n",
    "        if not cosine_recs.empty and not original_recs.empty:\n",
    "            avg_cosine_rating = cosine_recs['vote_average'].mean()\n",
    "            avg_original_rating = original_recs['vote_average'].mean()\n",
    "            \n",
    "            print(f\"  ‚≠ê Average Rating (Cosine): {avg_cosine_rating:.2f}\")\n",
    "            print(f\"  ‚≠ê Average Rating (Original): {avg_original_rating:.2f}\")\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'user_id': user_id,\n",
    "                'cosine_time': cosine_time,\n",
    "                'original_time': original_time,\n",
    "                'overlap': overlap,\n",
    "                'cosine_avg_rating': avg_cosine_rating,\n",
    "                'original_avg_rating': avg_original_rating\n",
    "            })\n",
    "        \n",
    "        # Show sample recommendations\n",
    "        print(\"  üéØ Top 3 Cosine Similarity Recommendations:\")\n",
    "        if not cosine_recs.empty:\n",
    "            for i, (_, movie) in enumerate(cosine_recs.head(3).iterrows()):\n",
    "                print(f\"    {i+1}. {movie['title']} ({movie['vote_average']:.1f})\")\n",
    "        else:\n",
    "            print(\"    None\")\n",
    "            \n",
    "        print(\"  üéØ Top 3 Original Algorithm Recommendations:\")\n",
    "        if not original_recs.empty:\n",
    "            for i, (_, movie) in enumerate(original_recs.head(3).iterrows()):\n",
    "                print(f\"    {i+1}. {movie['title']} ({movie['vote_average']:.1f})\")\n",
    "        else:\n",
    "            print(\"    None\")\n",
    "    \n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_with_original_algorithm()\n",
    "\n",
    "if not comparison_results.empty:\n",
    "    print(\"\\nüìà Overall Comparison Summary:\")\n",
    "    print(f\"Average Cosine Similarity Time: {comparison_results['cosine_time'].mean():.3f}s\")\n",
    "    print(f\"Average Original Algorithm Time: {comparison_results['original_time'].mean():.3f}s\")\n",
    "    print(f\"Average Recommendation Overlap: {comparison_results['overlap'].mean():.1f}/10\")\n",
    "    \n",
    "    comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Collaborative Filtering Evaluation Results', fontsize=16)\n",
    "\n",
    "# 1. Prediction Accuracy vs K\n",
    "if not accuracy_results.empty:\n",
    "    axes[0,0].plot(accuracy_results['k'], accuracy_results['MAE'], 'o-', label='MAE', linewidth=2)\n",
    "    axes[0,0].plot(accuracy_results['k'], accuracy_results['RMSE'], 's-', label='RMSE', linewidth=2)\n",
    "    axes[0,0].set_xlabel('Number of Similar Users (k)')\n",
    "    axes[0,0].set_ylabel('Error')\n",
    "    axes[0,0].set_title('Prediction Accuracy vs K')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Quality Metrics Distribution\n",
    "if not quality_results.empty:\n",
    "    axes[0,1].scatter(quality_results['user_avg_rating'], quality_results['avg_predicted_rating'], alpha=0.7)\n",
    "    axes[0,1].plot([5, 10], [5, 10], 'r--', alpha=0.5)  # Perfect prediction line\n",
    "    axes[0,1].set_xlabel('User Average Rating')\n",
    "    axes[0,1].set_ylabel('Average Predicted Rating')\n",
    "    axes[0,1].set_title('Prediction Quality by User')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Genre Diversity\n",
    "if not quality_results.empty:\n",
    "    axes[1,0].bar(range(len(quality_results)), quality_results['genre_diversity'])\n",
    "    axes[1,0].set_xlabel('User Index')\n",
    "    axes[1,0].set_ylabel('Number of Unique Genres')\n",
    "    axes[1,0].set_title('Recommendation Diversity by User')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Performance Comparison\n",
    "if not comparison_results.empty:\n",
    "    x_pos = np.arange(len(comparison_results))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,1].bar(x_pos - width/2, comparison_results['cosine_time'], width, label='Cosine Similarity')\n",
    "    axes[1,1].bar(x_pos + width/2, comparison_results['original_time'], width, label='Original Algorithm')\n",
    "    axes[1,1].set_xlabel('User Index')\n",
    "    axes[1,1].set_ylabel('Time (seconds)')\n",
    "    axes[1,1].set_title('Performance Comparison')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not accuracy_results.empty:\n",
    "    best_k = accuracy_results.loc[accuracy_results['MAE'].idxmin()]['k']\n",
    "    best_mae = accuracy_results['MAE'].min()\n",
    "    print(f\"üéØ Best Prediction Accuracy: MAE = {best_mae:.3f} at k = {best_k}\")\n",
    "\n",
    "if not quality_results.empty:\n",
    "    avg_diversity = quality_results['genre_diversity'].mean()\n",
    "    print(f\"üé≠ Average Genre Diversity: {avg_diversity:.1f} unique genres per user\")\n",
    "\n",
    "if not comparison_results.empty:\n",
    "    speedup = comparison_results['original_time'].mean() / comparison_results['cosine_time'].mean()\n",
    "    print(f\"‚ö° Performance: Pure cosine similarity is {speedup:.1f}x {'faster' if speedup > 1 else 'slower'} than original\")\n",
    "    avg_overlap = comparison_results['overlap'].mean()\n",
    "    print(f\"üîÑ Average Recommendation Overlap: {avg_overlap:.1f}/10 movies\")\n",
    "\n",
    "print(f\"\\nüíæ Memory Usage: {perf_results['matrix_memory_mb']:.1f} MB for user-item matrix\")\n",
    "print(f\"‚è±Ô∏è  Average Recommendation Time: {perf_results['recommendation_time_per_user']:.3f}s per user\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "\n",
    "1. **Prediction Accuracy**: \n",
    "   - Pure cosine similarity provides reasonable prediction accuracy\n",
    "   - Optimal k value (number of similar users) varies based on data sparsity\n",
    "   - Performance degrades with very high or very low k values\n",
    "\n",
    "2. **Recommendation Quality**:\n",
    "   - Cosine similarity tends to recommend movies similar to user preferences\n",
    "   - Good diversity in genres when sufficient user data is available\n",
    "   - Quality depends heavily on the number of ratings per user\n",
    "\n",
    "3. **Performance**:\n",
    "   - Pure cosine similarity is computationally efficient for similarity calculation\n",
    "   - Memory usage scales with number of users and items\n",
    "   - Performance bottleneck is in rating prediction for many items\n",
    "\n",
    "4. **Comparison with Original Algorithm**:\n",
    "   - Pure cosine similarity provides different but valid recommendations\n",
    "   - Original hybrid approach may provide more diverse recommendations\n",
    "   - Trade-off between simplicity and recommendation quality\n",
    "\n",
    "### Recommendations for Improvement:\n",
    "\n",
    "1. **Optimize k value**: Use cross-validation to find optimal k for your dataset\n",
    "2. **Handle cold start**: Implement content-based fallback for new users\n",
    "3. **Sparse data**: Consider matrix factorization for better handling of sparse ratings\n",
    "4. **Performance**: Implement approximate similarity for large-scale systems\n",
    "5. **Diversity**: Add diversity constraints to avoid over-specialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for further analysis\n",
    "print(\"üíæ Saving evaluation results...\")\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('evaluation_results', exist_ok=True)\n",
    "\n",
    "# Save all results\n",
    "if not accuracy_results.empty:\n",
    "    accuracy_results.to_csv('evaluation_results/accuracy_results.csv', index=False)\n",
    "    print(\"‚úÖ Accuracy results saved\")\n",
    "\n",
    "if not quality_results.empty:\n",
    "    quality_results.to_csv('evaluation_results/quality_results.csv', index=False)\n",
    "    print(\"‚úÖ Quality results saved\")\n",
    "\n",
    "if not comparison_results.empty:\n",
    "    comparison_results.to_csv('evaluation_results/comparison_results.csv', index=False)\n",
    "    print(\"‚úÖ Comparison results saved\")\n",
    "\n",
    "# Save performance metrics\n",
    "with open('evaluation_results/performance_metrics.json', 'w') as f:\n",
    "    import json\n",
    "    json.dump(perf_results, f, indent=2)\n",
    "    print(\"‚úÖ Performance metrics saved\")\n",
    "\n",
    "print(\"\\nüéâ All evaluation results saved to 'evaluation_results' directory!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}